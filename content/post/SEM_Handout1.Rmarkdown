---
title: "Structural Equation Modeling in R Tutorial 1: Two predictor regression using R"
authors: ["admin"]
date: 2017-08-21
tags: ['structural equation modeling', 'regression', 'matrix algebra', 'R']
categories: ['structural equation modeling', 'regression', 'matrix algebra', 'R']
output: blogdown::html_page
---

{{% toc %}}

Made for Jonathan Butner's Structural Equation Modeling (SEM) Class, Fall 2017, University of Utah. This handout begins by showing how to import data into R. Then, correlation matrices are generated, followed by a two predictor regression analysis. Lastly, it is shown how to output a matrix as an external file and use it for regression. This syntax imports the 52 cases from datafile Grade230.txt. 

```{r include=FALSE}
library(knitr)
opts_chunk$set(tidy.opts=list(width.cutoff=60),tidy=TRUE)
options(scipen=999)
```



## Data Input and Cleaning
First, we'll load required packages.
```{r, message=FALSE, warning=FALSE}
##this calls the packages required. use install.packages('packagename') if not installed.

library(dplyr) #for cleaning data
library(car) #for scatterplotting
library(Hmisc) #for significance of correlation coefficients
library(psych) #for doing regression from a correlation matrix, getting descriptives
library(scatterplot3d) #for making 3d scatterplots
library(MASS) #for saving some casewise diagnostics
library(lavaan) #doing regression from a covariance matrix, eventually we will use this for SEM

```
Then, we will read in datafile using Fortran and clean up the datafile a little bit.
```{r}
#make sure to set your working directory to where the file is located, e.g. setwd("D:/Downloads/Handout 1 - Regression")
#you can do this in R Studio by going to the Session menu - "Set Working Directory" - To Source File Location
grades <- read.fortran("GRADE230.txt", 
                       c("F3.0","F5.0","F24.0", "7F2.0", "F3.0", "2F2.0", "F4.0", "4F2.0", "F4.0"))
#note: #The format for a field is of one of the following forms: rFl.d, rDl.d, rXl, rAl, rIl, where l is the number of columns, d is the number of decimal places, and r is the number of repeats. F and D are numeric formats, A is character, I is integer, and X indicates columns to be skipped. The repeat code r and decimal place code d are always optional. The length code l is required except for X formats when r is present.

#add column names
names(grades) <- c("case", "sex", "T1", "PS1", "PS2", "PS3", "PS4","PS5", "PS6", "PS7", "T2", "PS8", "PS9", "T3", "PS10", "PS11", "PS12", "PS13", "T4")

#selects a subset of the data for analysis, stores in new dataframe
grades.sub <- subset(grades, 
                     grades$case < 21 & grades$case != 9)
#!= indicates does not equal

#lets take a look at the datafile
grades.sub #note that R treats blank cells in your original data as missing, and labels these cases NA. NA is the default 

#subset out specific tests using dplyr
grades.test <- dplyr::select(grades.sub, 
                             c(T1, T2, T4))


#get descriptives using psych package
describe(grades.test)

```
Note that R treats blank cells in your original data as missing, and labels these cases NA. NA is the default missing data label implemented by R. 

<br>
## Create and Export a Correlation Matrix
Now, we'll create a correlation matrix and show you how to export a correlation matrix to an external file on your computer harddrive. Note that the first correlation matrix created uses the option "pairwise.complete.obs", which implements pairwiise deletion for missing data. This is usually undesirable, as it deletes variables, not entire cases, and thus can bias paramter estimates. The second option, "complete obs", implements listwise deletion for missing data, which is more desirable than pairwise deletion because parameter estimates are less biased (entire cases are deleted, not just specific variables).
```{r}
#create a correlation matrix amongst variables
grades.cor <- cor(grades.test, 
                  use="pairwise.complete.obs", 
                  method="pearson")
grades.cor #print the correlation matrix
rcorr(as.matrix(grades.test)) #need if you want significance of correlations, but only uses pairwise. From Hmisc pacakge. 
```
```{r eval = FALSE}
#save out the correlation matrix to a file
write.csv(grades.cor, "corroutPW.csv")
```
```{r}
grades.cor <- cor(grades.test, use="complete.obs", method="pearson")
grades.cor #notice the differences when we use listwise deletion
```
```{r eval=FALSE}
#save out correlation matrix to a file on your harddrive
write.csv(grades.cor, "corroutLW.csv")
```

<br>
## Multiple Regression
Now, we'll do some multiple regression. Specifically we'll ask whether tests 1 and 2 predict test 4. We'll also check some model assumptions, including whether or not there are outliers present and whether or not there is multicollinearity amongst tests (Variance inflation factor, or VIF). Some of this code helps you to save out residuals, predicted values, and other casewise diagnostics to a dataframe for later inspection. Note that the lm command defaults to listwise deletion. 

```{r}
model <- lm(T4 ~ T1 + T2, data=grades.test)
#print out model results
anova(model) 
summary(model)

#save out fitted values and predicted values to dataframe
grades.test$Predicted <- predict(model) 
grades.test$Residuals <- residuals(model)

#save out casewise diagnostics (outliers)
grades.test$leverage <- hatvalues(model)
grades.test$distance <- studres(model)
grades.test$dffits <- dffits(model) #measure of global influence 
grades.test$dfbetas <- dfbetas(model) #measure of specific influence

#test of multicollinearity
vif(model) 

#or you can use a more general command for casewise diagnostics:
influence.measures(model)

model.cov <- vcov(model) #save variance covariance matrix for cofficients
grades.cov <- cov(grades.test) #save covariance matrix from raw data
```
**Model results and what they mean:**

1. **Multiple R-squared** tells you the porportion of variance in the dependent variable that is predicted or accounted for given the linear combination of the independent variables in your model.

2. **Adjusted R-squared** tells you an estimate of the population level R-squared value.

3. **Residual standard error** tells you the average standard deviation of the residuals (raw metric). If squared is the mean square error (MSE), included in the anova table next to residual.

4. **Significance term following F-statistic** provide an omnibus test against an intercept only model with no predictors (does your model predict your outcome better than just a mean?)

5. **Analysis of Variance table Mean Sq** the variance of the residuals

6. **Variance inflation factor** tells you whether there is multicollinearity amongst predictors in your model. Usually a number greater than 10 indicates a problem. Lower is better.

7. **Influence measures** provide a number of casewise diagnostics. In this output, the corresponding column numbers in their respective order indicate: dfbeta of the intercept, dfbeta of X1 , dfbeta of x2, dffits (global influence, or how much Yhat (predicted Y) changes based on removal of a case), covariance ratio (the change in the determinant of the covariance matrix of the estimates by deleting this observation), cook's distance (also influence), leverage (how unusual is the observation in terms of its values on the independent predictors?), significance test marking that case as a potential outlier. Note that one way to spot outliers is to look for residuals that are more than 2 SDs beyond the mean (mean is always 0). See [this webpage](http://ianruginski.com/regressionassumptionswithR_tutorial.html) I made for more on checking the assumptions of regression. 

Next, let's make some plots of the models. 
```{r}
#making plots of the model using car package
scatterplot(T4 ~ T1, data=grades.test)
scatterplot(Residuals ~ T1, data=grades.test)
```
The green line indicates a linear best fit while the red line shows a loess fit. The dashed red line indicates +-1 standard error of the loess smoothed fitted line. The extra arguments to the first scatterplot command label each datapoint to help with outlier identification. Note for the second plot that, if the residuals were normally distributed, we would have a flat line rather than a curved line.

<br>
## Using Multiple Regression to show how coefficients are a function of residuals
Now, lets look at how the coefficients are a function of residuals. We will build the coefficient for T1 from the previous regression. First we will create a residual of T4 (the criterion) controlling for the predictors other than T1.

```{r}
model.t4 <- lm(T4~T2, data=grades.test)
grades.test$model.t4resid <- residuals(model.t4) #saves residuals to original dataframe
```

Next we create residuals for T1 (the predictor) controlling for the predictors other than T1. We regress T1 on T2, giving us Y=b0+b1T2 where Y is T1. What remains (the residuals, which we save out) is everything unrelated to T2.

```{r}
model.t1 <- lm(T1~T2, data=grades.test)
grades.test$model.t1resid <- residuals(model.t1) #saves residuals to original dataframe

```

Now we run the regression using T4 with all of T2 removed as the DV and T1 with all of T2 removed as the independent variable.

```{r}
model.final <- lm(model.t4resid ~ model.t1resid, data=grades.test)
anova(model.final)
summary(model.final) #print model results
```

Note that the regression coefficient is identical to the coefficient in the two predictor regression from earlier. 
Next, we're going to run another regression with the case as the DV. We will create a new graph to show that leverage is only dependent on the predictor and not the dependent variable.

```{r}
model.leverage <- lm(case ~ T1 + T2, data=grades.sub)
grades.sub$leverage <- hatvalues(model.leverage)
anova(model.leverage)
summary(model.leverage)
scatterplot(leverage ~ case, data=grades.sub)
```

Note that in SEM there is no simple distance or leverage measures, but we can get at leverage as it is separate from the DV. If we can identify a case that is extreme we run the analyses with and without the case to determine its impact. The change in the output would be the test of leverage.

Now we will make a 3d scatterplot of relationship between tests.

```{r}
s3d <- scatterplot3d(grades.test$T1, grades.test$T2, grades.test$T4 , pch=16, highlight.3d=TRUE, type="h", main="3D Scatterplot")
s3d$plane3d(model) #uses our model from earlier to draw a regression plane
```

<br>
##Multiple regression using a correlation matrix

Now we will show how to do regression using just a correlation matrix. This is extremely useful if you want to do additional analyses on existing papers that provide a correlation and/or covariance matrix, but you do not have access to the raw data from those papers.

```{r}
#call in correlation matrix from file on your computer.
cormatlw <- read.csv("corroutLW.csv", row.names = 1) #tells R that first column contains row names
cormatlw <- data.matrix(cormatlw) #change from dataframe to matrix

#do regression with the correlation matrix without raw data
model.cor <- setCor(y = "T4", x = c("T1", "T2"), data=cormatlw, n.obs=18, std=FALSE)
summary(model.cor)
```
